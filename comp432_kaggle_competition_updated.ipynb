{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimNafiz/kaggle_competition_comp432/blob/main/comp432_kaggle_competition_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###All necessary imports"
      ],
      "metadata": {
        "id": "PWjVUx7ibeHz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bh0Iv2rQbREq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.modules import linear\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helpers"
      ],
      "metadata": {
        "id": "wr9MC6YfdXJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_misclassifications_per_label(model, data_loader, device):\n",
        "    model.eval()\n",
        "    classified_dict = {}\n",
        "    misclassified_dict = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs,1)\n",
        "          for pred, truth in zip(predicted, targets):\n",
        "            if pred == truth:\n",
        "              if pred.item() in classified_dict:\n",
        "                classified_dict[pred.item()] += 1\n",
        "              else:\n",
        "                classified_dict[pred.item()] = 1\n",
        "            else:\n",
        "              if pred.item() in misclassified_dict:\n",
        "                misclassified_dict[pred.item()] += 1\n",
        "              else:\n",
        "                misclassified_dict[pred.item()] = 1\n",
        "\n",
        "    return classified_dict, misclassified_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "zNgxzTLbxrW0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_label_ratio(y):\n",
        "  # convert to np array\n",
        "  # not doing any error handling don't have the time\n",
        "  y_np = np.array(y)\n",
        "  y_count = len(y_np)\n",
        "  unique, counts = np.unique(y_np, return_counts=True)\n",
        "  ratio = counts / y_count\n",
        "  return (unique, ratio)"
      ],
      "metadata": {
        "id": "F-wkV5ChZnaJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_init_weights_normal(mean, std):\n",
        "  def init_weights_normal(model):\n",
        "    if type(model) == nn.Linear:\n",
        "      nn.init.normal_(model.weight, mean = mean, std = std )\n",
        "      if model.bias is not None:\n",
        "        nn.init.constant_(model.bias, 0)\n",
        "  return init_weights_normal"
      ],
      "metadata": {
        "id": "Qk6dMILnBqpV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_weights(model, num_values=10):\n",
        "    \"\"\"\n",
        "    Prints the weight and bias statistics for each Linear layer in the model.\n",
        "    num_values: how many values from the weights to show.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== MODEL WEIGHTS SUMMARY ===\")\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            flat_w = module.weight.view(-1)\n",
        "\n",
        "            print(f\"\\nLayer: {name} ({module.__class__.__name__})\")\n",
        "            print(f\"  Weight shape: {module.weight.shape}\")\n",
        "            print(f\"  Bias shape:   {module.bias.shape if module.bias is not None else None}\")\n",
        "            print(f\"  First {num_values} weight values: {flat_w[:num_values].tolist()}\")\n",
        "            print(f\"  Mean: {float(flat_w.mean()):.6f}, Std: {float(flat_w.std()):.6f}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "    print(\"=== END OF WEIGHTS SUMMARY ===\\n\")"
      ],
      "metadata": {
        "id": "tIEUSmXjCpIu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # ---- Loss Plot ----\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Accuracy Plot ----\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "J8UPAgOCcIDm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_and_save_history(history, out_dir):\n",
        "    \"\"\"Plot loss/accuracy and save to files in out_dir.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # ---- Loss Plot ----\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    loss_path = os.path.join(out_dir, \"loss_curve.png\")\n",
        "    plt.savefig(loss_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    # ---- Accuracy Plot ----\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    acc_path = os.path.join(out_dir, \"accuracy_curve.png\")\n",
        "    plt.savefig(acc_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    return loss_path, acc_path\n",
        "\n",
        "\n",
        "def log_experiment(\n",
        "    config: dict,\n",
        "    history: dict,\n",
        "    test_results: dict,\n",
        "    log_root: str = \"experiments\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a timestamped folder and logs:\n",
        "      - config.json       (hyperparameters + setup)\n",
        "      - metrics.json      (history + test_results)\n",
        "      - loss_curve.png\n",
        "      - accuracy_curve.png\n",
        "    Returns the path to the experiment folder.\n",
        "    \"\"\"\n",
        "    # 1) Make root dir if needed\n",
        "    os.makedirs(log_root, exist_ok=True)\n",
        "\n",
        "    # 2) Unique folder name: timestamp + short tag\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    exp_name = f\"run_{timestamp}\"\n",
        "    exp_dir = os.path.join(log_root, exp_name)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    # 3) Save config (hyperparameters, ratios, etc.)\n",
        "    config_path = os.path.join(exp_dir, \"config.json\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    # 4) Save metrics (training history + test)\n",
        "    metrics = {\n",
        "        \"history\": history,\n",
        "        \"test_results\": test_results,\n",
        "    }\n",
        "    metrics_path = os.path.join(exp_dir, \"metrics.json\")\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "\n",
        "    # 5) Save plots\n",
        "    loss_path, acc_path = plot_and_save_history(history, exp_dir)\n",
        "\n",
        "    print(f\"[LOG] Experiment saved to: {exp_dir}\")\n",
        "    print(f\"  - Config:   {config_path}\")\n",
        "    print(f\"  - Metrics:  {metrics_path}\")\n",
        "    print(f\"  - Loss plot: {loss_path}\")\n",
        "    print(f\"  - Acc plot:  {acc_path}\")\n",
        "\n",
        "    return exp_dir\n"
      ],
      "metadata": {
        "id": "LpQjbSzAfxua"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fully Connected Model\n"
      ],
      "metadata": {
        "id": "Co_zNdGsbsZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HouseObjectClassifier(nn.Module):\n",
        "  def __init__(self, input_features, output_class):\n",
        "    super().__init__()\n",
        "    # we have 500 features\n",
        "    self.linear_layer1 = nn.Linear(input_features, 300)\n",
        "    self.linear_layer2 = nn.Linear(300, 150)\n",
        "    # self.linear_layer3 = nn.Linear(256, 128)\n",
        "    self.output_layer = nn.Linear(150, output_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.linear_layer1(x))\n",
        "    x = torch.relu(self.linear_layer2(x))\n",
        "    # x = torch.relu(self.linear_layer3(x))\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "# I will try them out later\n",
        "\n",
        "# class HouseObjectClassifier(nn.Module):\n",
        "#   def __init__(self, input_features, output_class):\n",
        "#     super().__init__()\n",
        "#     self.linear_layer1 = nn.Linear(input_features, 512)\n",
        "#     self.linear_layer2 = nn.Linear(512, 256)\n",
        "#     self.linear_layer3 = nn.Linear(256, 128)\n",
        "#     self.output_layer = nn.Linear(128, output_class)\n",
        "\n",
        "#     self.dropout1 = nn.Dropout(p=0.5)\n",
        "#     self.dropout2 = nn.Dropout(p=0.5)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = torch.relu(self.linear_layer1(x))\n",
        "#     x = self.dropout1(x)\n",
        "#     x = torch.relu(self.linear_layer2(x))\n",
        "#     x = self.dropout2(x)\n",
        "#     x = torch.relu(self.linear_layer3(x))\n",
        "#     x = self.output_layer(x)\n",
        "#     return x"
      ],
      "metadata": {
        "id": "frbRahevbjMM"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function for initializing weights from a normal distribution"
      ],
      "metadata": {
        "id": "Zrhw3vIadQY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_init_weights_normal(mean, std):\n",
        "  def init_weights_normal(model):\n",
        "    if type(model) == nn.Linear:\n",
        "      nn.init.normal_(model.weight, mean = mean, std = std )\n",
        "      if model.bias is not None:\n",
        "        nn.init.constant_(model.bias, 0)\n",
        "  return init_weights_normal"
      ],
      "metadata": {
        "id": "SQyhdnpDdQJN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function for loading the dataset and converting it into tensors"
      ],
      "metadata": {
        "id": "GVjoIBZkchHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(csv_path: str, drop_columns=None):\n",
        "    \"\"\"\n",
        "    Loads the CSV, drops unwanted columns, and returns:\n",
        "      X: float tensor of shape (N, num_features)\n",
        "      y: long tensor of shape (N,)\n",
        "    Assumes the LAST remaining column is the label.\n",
        "    \"\"\"\n",
        "    if drop_columns is None:\n",
        "        drop_columns = []\n",
        "\n",
        "    df = pd.read_csv(csv_path, engine=\"python\", quotechar='\"', escapechar='\\\\')\n",
        "    df = df.drop(columns=drop_columns, errors=\"ignore\")\n",
        "\n",
        "    data_np = df.values\n",
        "    X_np = data_np[:, :-1]\n",
        "    y_np = data_np[:, -1]\n",
        "\n",
        "    X = torch.tensor(X_np, dtype=torch.float32)\n",
        "    y = torch.tensor(y_np, dtype=torch.long)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "fz-k3NFScfol"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for creating DataLoaders (required for batching and making life easier)"
      ],
      "metadata": {
        "id": "vjqes465cqtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(\n",
        "    train_X_img, train_y,\n",
        "    val_X_img, val_y,\n",
        "    test_X_img, test_y,\n",
        "    batch_size: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Wraps tensors in TensorDatasets and DataLoaders.\n",
        "    \"\"\"\n",
        "    train_dataset = TensorDataset(train_X_img, train_y)\n",
        "    val_dataset   = TensorDataset(val_X_img,   val_y)\n",
        "    test_dataset  = TensorDataset(test_X_img,  test_y)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "yMGQJAVKcp9U"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions for normalization"
      ],
      "metadata": {
        "id": "YRinjHmhb0g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i might have to re-consider, the normalization strategy\n",
        "# this function destroyed my model\n",
        "def compute_normalization(train_X: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Computes feature-wise mean and std from TRAIN data only.\n",
        "    \"\"\"\n",
        "    mean = train_X.mean(dim=0)\n",
        "    std  = train_X.std(dim=0) + 1e-6   # avoid division by zero\n",
        "    #print(f\"mean {mean} std {std}\")\n",
        "    print(f\"mean shape {mean.shape} std shape {std.shape}\")\n",
        "    return mean, std\n",
        "\n",
        "def log1p_transform(X):\n",
        "  return torch.log1p(X)\n",
        "\n",
        "\n",
        "\n",
        "def apply_normalization(X: torch.Tensor, mean: torch.Tensor, std: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Applies (X - mean) / std feature-wise.\n",
        "    \"\"\"\n",
        "    return (X - mean) / std"
      ],
      "metadata": {
        "id": "wCDlFKQWbr2z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function for splitting the data into train, val and test"
      ],
      "metadata": {
        "id": "bGALNiy5b7uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default dict makes my life much easies don't have to think about cases where the key doesn't contain a value\n",
        "# I realized later this is not important, the data already is very balanced, but better to be safe than sorrow\n",
        "from collections import defaultdict\n",
        "\n",
        "def stratified_train_val_test_split(X, y, train_ratio: float, val_ratio: float, test_ratio: float):\n",
        "    \"\"\"\n",
        "    Returns stratified splits of X, y.\n",
        "\n",
        "    Ensures each class appears in train/val/test according to the given ratios.\n",
        "    \"\"\"\n",
        "    total = train_ratio + val_ratio + test_ratio\n",
        "    if abs(total - 1.0) > 1e-6:\n",
        "        raise ValueError(\"train_ratio + val_ratio + test_ratio must equal 1.\")\n",
        "\n",
        "    # Collect indices for each class\n",
        "    class_indices = defaultdict(list)\n",
        "    for idx, label in enumerate(y):\n",
        "        class_indices[int(label.item())].append(idx)\n",
        "\n",
        "    train_idx = []\n",
        "    val_idx = []\n",
        "    test_idx = []\n",
        "\n",
        "    # For each class, split indices by ratio\n",
        "    for cls, indices in class_indices.items():\n",
        "        indices = torch.tensor(indices)\n",
        "        indices = indices[torch.randperm(len(indices))]  # shuffle within class\n",
        "\n",
        "        n = len(indices)\n",
        "        n_train = int(n * train_ratio)\n",
        "        n_val = int(n * val_ratio)\n",
        "        n_test = n - n_train - n_val  # rest goes to test\n",
        "\n",
        "        train_idx.append(indices[:n_train])\n",
        "        val_idx.append(indices[n_train:n_train+n_val])\n",
        "        test_idx.append(indices[n_train+n_val:])\n",
        "\n",
        "    # Concatenate all classes\n",
        "    train_idx = torch.cat(train_idx)\n",
        "    val_idx = torch.cat(val_idx)\n",
        "    test_idx = torch.cat(test_idx)\n",
        "\n",
        "    # Shuffle each split (optional but recommended)\n",
        "    train_idx = train_idx[torch.randperm(len(train_idx))]\n",
        "    val_idx = val_idx[torch.randperm(len(val_idx))]\n",
        "    test_idx = test_idx[torch.randperm(len(test_idx))]\n",
        "\n",
        "    # Gather actual data\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_val,   y_val   = X[val_idx],   y[val_idx]\n",
        "    X_test,  y_test  = X[test_idx],  y[test_idx]\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "# DEPRICATED\n",
        "def train_val_test_split(X, y, train_ratio: float, val_ratio: float, test_ratio: float):\n",
        "    \"\"\"\n",
        "    Splits X, y into train/val/test using the provided ratios.\n",
        "    \"\"\"\n",
        "    total = train_ratio + val_ratio + test_ratio\n",
        "    if total != 1.0:\n",
        "      raise Exception(\"train val and test ratio don't add upto 1\")\n",
        "\n",
        "\n",
        "    N = X.size(0)\n",
        "    indices = torch.randperm(N)\n",
        "\n",
        "    train_size = int(N * train_ratio)\n",
        "    val_size   = int(N * val_ratio)\n",
        "    test_size  = N - train_size - val_size\n",
        "\n",
        "    train_idx = indices[:train_size]\n",
        "    val_idx   = indices[train_size:train_size + val_size]\n",
        "    test_idx  = indices[train_size + val_size:]\n",
        "\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_val,   y_val   = X[val_idx],   y[val_idx]\n",
        "    X_test,  y_test  = X[test_idx],  y[test_idx]\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n"
      ],
      "metadata": {
        "id": "H7VFrG2tb3ac"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###functions for training"
      ],
      "metadata": {
        "id": "AsfnBTK1c4YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        # inputs: (batch, H, W) -> add channel dim\n",
        "        #inputs = inputs.unsqueeze(1).to(device)   # (batch, 1, H, W)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # tf is this?\n",
        "        running_loss += loss.item() * targets.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        total   += targets.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "7s8Vd_JoZfAP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            #inputs = inputs.unsqueeze(1).to(device)  # (batch, 1, H, W)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * targets.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total   += targets.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "BdyYkt4XZiDo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    num_epochs: int,\n",
        "    device\n",
        "):\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\":  [],\n",
        "        \"val_loss\":   [],\n",
        "        \"val_acc\":    [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss,   val_acc   = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch}/{num_epochs}] \"\n",
        "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
        "            f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "5UkCZg8VZjgY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TRAINING"
      ],
      "metadata": {
        "id": "bEUcOsEQd4lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important constants\n",
        "TRAIN_CSV_PATH = \"drive_karim/MyDrive/kaggle_competition_comp432_datasets/train.csv\"\n",
        "DROP_COLUMNS = [\"id\"]              # columns to drop from CSV\n",
        "\n",
        "TRAIN_RATIO = 0.6\n",
        "VAL_RATIO   = 0.2\n",
        "TEST_RATIO  = 0.2\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_CLASSES = 50\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0\n",
        "NUM_EPOCHS = 100\n",
        "log_root=\"/content/drive_karim/MyDrive/experiments\"\n",
        "# redifing batch size, cuz I am not happy with 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# FOR THE MODEL\n",
        "INIT_WEIGHTS_MEAN = 0.0\n",
        "INIT_WEIGHTS_STD = 0.02\n",
        "IS_INIT_WEIGHTS_NORMAL = True\n",
        "\n"
      ],
      "metadata": {
        "id": "b4eq2O_OehEs"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive_karim')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fx-aKV8nXEE",
        "outputId": "8df90c7b-fcc0-4737-9158-6474891dd1ed"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive_karim; to attempt to forcibly remount, call drive.mount(\"/content/drive_karim\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y = load_dataset(TRAIN_CSV_PATH, drop_columns=DROP_COLUMNS)"
      ],
      "metadata": {
        "id": "7FVH4g1TefXV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "train_split, val_split, test_split = stratified_train_val_test_split(X, y, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO, test_ratio=TEST_RATIO)\n",
        "label_ratios = [check_label_ratio(train_split[1]),check_label_ratio(val_split[1]),check_label_ratio(test_split[1])]\n",
        "for split in label_ratios:\n",
        "  unique, ratio = split\n",
        "  for label, ratio in zip(unique,ratio):\n",
        "    print(f\"label {label} ratio {ratio*100}\")"
      ],
      "metadata": {
        "id": "gDEx6Mwkd58l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc53272-688e-4ff9-aca1-759006c564aa"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label 0 ratio 2.000606682170767\n",
            "label 1 ratio 1.9977177194528306\n",
            "label 2 ratio 1.9991622008117986\n",
            "label 3 ratio 2.000606682170767\n",
            "label 4 ratio 2.000606682170767\n",
            "label 5 ratio 2.000606682170767\n",
            "label 6 ratio 2.002051163529735\n",
            "label 7 ratio 1.9991622008117986\n",
            "label 8 ratio 1.9962732380938624\n",
            "label 9 ratio 2.000606682170767\n",
            "label 10 ratio 1.9991622008117986\n",
            "label 11 ratio 1.9991622008117986\n",
            "label 12 ratio 2.002051163529735\n",
            "label 13 ratio 1.9991622008117986\n",
            "label 14 ratio 1.9991622008117986\n",
            "label 15 ratio 1.9948287567348943\n",
            "label 16 ratio 1.9977177194528306\n",
            "label 17 ratio 2.000606682170767\n",
            "label 18 ratio 2.000606682170767\n",
            "label 19 ratio 2.000606682170767\n",
            "label 20 ratio 2.002051163529735\n",
            "label 21 ratio 1.9991622008117986\n",
            "label 22 ratio 2.002051163529735\n",
            "label 23 ratio 2.000606682170767\n",
            "label 24 ratio 2.000606682170767\n",
            "label 25 ratio 1.9962732380938624\n",
            "label 26 ratio 1.9991622008117986\n",
            "label 27 ratio 2.002051163529735\n",
            "label 28 ratio 2.000606682170767\n",
            "label 29 ratio 2.002051163529735\n",
            "label 30 ratio 2.002051163529735\n",
            "label 31 ratio 1.9991622008117986\n",
            "label 32 ratio 2.002051163529735\n",
            "label 33 ratio 2.002051163529735\n",
            "label 34 ratio 2.000606682170767\n",
            "label 35 ratio 2.000606682170767\n",
            "label 36 ratio 1.9991622008117986\n",
            "label 37 ratio 1.9977177194528306\n",
            "label 38 ratio 2.000606682170767\n",
            "label 39 ratio 1.9991622008117986\n",
            "label 40 ratio 2.002051163529735\n",
            "label 41 ratio 2.002051163529735\n",
            "label 42 ratio 1.9991622008117986\n",
            "label 43 ratio 1.9991622008117986\n",
            "label 44 ratio 1.9977177194528306\n",
            "label 45 ratio 2.002051163529735\n",
            "label 46 ratio 1.9991622008117986\n",
            "label 47 ratio 1.9977177194528306\n",
            "label 48 ratio 2.002051163529735\n",
            "label 49 ratio 2.000606682170767\n",
            "label 0 ratio 1.9991326973113617\n",
            "label 1 ratio 1.9991326973113617\n",
            "label 2 ratio 1.9991326973113617\n",
            "label 3 ratio 1.9991326973113617\n",
            "label 4 ratio 1.9991326973113617\n",
            "label 5 ratio 1.9991326973113617\n",
            "label 6 ratio 2.0034692107545533\n",
            "label 7 ratio 1.9991326973113617\n",
            "label 8 ratio 1.99479618386817\n",
            "label 9 ratio 1.9991326973113617\n",
            "label 10 ratio 1.9991326973113617\n",
            "label 11 ratio 1.9991326973113617\n",
            "label 12 ratio 2.0034692107545533\n",
            "label 13 ratio 1.9991326973113617\n",
            "label 14 ratio 1.9991326973113617\n",
            "label 15 ratio 1.99479618386817\n",
            "label 16 ratio 1.9991326973113617\n",
            "label 17 ratio 1.9991326973113617\n",
            "label 18 ratio 1.9991326973113617\n",
            "label 19 ratio 1.9991326973113617\n",
            "label 20 ratio 2.0034692107545533\n",
            "label 21 ratio 1.9991326973113617\n",
            "label 22 ratio 2.0034692107545533\n",
            "label 23 ratio 1.9991326973113617\n",
            "label 24 ratio 1.9991326973113617\n",
            "label 25 ratio 1.99479618386817\n",
            "label 26 ratio 1.9991326973113617\n",
            "label 27 ratio 2.0034692107545533\n",
            "label 28 ratio 1.9991326973113617\n",
            "label 29 ratio 2.0034692107545533\n",
            "label 30 ratio 2.0034692107545533\n",
            "label 31 ratio 1.9991326973113617\n",
            "label 32 ratio 2.0034692107545533\n",
            "label 33 ratio 2.0034692107545533\n",
            "label 34 ratio 1.9991326973113617\n",
            "label 35 ratio 1.9991326973113617\n",
            "label 36 ratio 1.9991326973113617\n",
            "label 37 ratio 1.9991326973113617\n",
            "label 38 ratio 1.9991326973113617\n",
            "label 39 ratio 1.9991326973113617\n",
            "label 40 ratio 2.0034692107545533\n",
            "label 41 ratio 2.0034692107545533\n",
            "label 42 ratio 1.9991326973113617\n",
            "label 43 ratio 1.9991326973113617\n",
            "label 44 ratio 1.9991326973113617\n",
            "label 45 ratio 2.0034692107545533\n",
            "label 46 ratio 1.9991326973113617\n",
            "label 47 ratio 1.9991326973113617\n",
            "label 48 ratio 2.0034692107545533\n",
            "label 49 ratio 1.9991326973113617\n",
            "label 0 ratio 2.002855041744171\n",
            "label 1 ratio 1.9985292209196697\n",
            "label 2 ratio 1.9985292209196697\n",
            "label 3 ratio 2.002855041744171\n",
            "label 4 ratio 2.002855041744171\n",
            "label 5 ratio 2.002855041744171\n",
            "label 6 ratio 1.9985292209196697\n",
            "label 7 ratio 1.9985292209196697\n",
            "label 8 ratio 1.9985292209196697\n",
            "label 9 ratio 2.002855041744171\n",
            "label 10 ratio 1.9985292209196697\n",
            "label 11 ratio 1.9985292209196697\n",
            "label 12 ratio 1.9985292209196697\n",
            "label 13 ratio 2.002855041744171\n",
            "label 14 ratio 1.9985292209196697\n",
            "label 15 ratio 1.9985292209196697\n",
            "label 16 ratio 1.994203400095168\n",
            "label 17 ratio 2.002855041744171\n",
            "label 18 ratio 2.002855041744171\n",
            "label 19 ratio 2.002855041744171\n",
            "label 20 ratio 1.9985292209196697\n",
            "label 21 ratio 1.9985292209196697\n",
            "label 22 ratio 1.9985292209196697\n",
            "label 23 ratio 2.002855041744171\n",
            "label 24 ratio 2.002855041744171\n",
            "label 25 ratio 1.9985292209196697\n",
            "label 26 ratio 2.002855041744171\n",
            "label 27 ratio 1.9985292209196697\n",
            "label 28 ratio 2.002855041744171\n",
            "label 29 ratio 1.9985292209196697\n",
            "label 30 ratio 1.9985292209196697\n",
            "label 31 ratio 2.002855041744171\n",
            "label 32 ratio 1.9985292209196697\n",
            "label 33 ratio 1.9985292209196697\n",
            "label 34 ratio 2.002855041744171\n",
            "label 35 ratio 2.002855041744171\n",
            "label 36 ratio 1.9985292209196697\n",
            "label 37 ratio 1.994203400095168\n",
            "label 38 ratio 2.002855041744171\n",
            "label 39 ratio 1.9985292209196697\n",
            "label 40 ratio 1.9985292209196697\n",
            "label 41 ratio 1.9985292209196697\n",
            "label 42 ratio 2.002855041744171\n",
            "label 43 ratio 2.002855041744171\n",
            "label 44 ratio 1.9985292209196697\n",
            "label 45 ratio 1.9985292209196697\n",
            "label 46 ratio 1.9985292209196697\n",
            "label 47 ratio 1.994203400095168\n",
            "label 48 ratio 1.9985292209196697\n",
            "label 49 ratio 2.002855041744171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create a model instance\n",
        "model = HouseObjectClassifier(input_features=500, output_class=NUM_CLASSES).to(device)\n",
        "if(IS_INIT_WEIGHTS_NORMAL):\n",
        "  # applying closure\n",
        "  model.apply(get_init_weights_normal(INIT_WEIGHTS_MEAN,INIT_WEIGHTS_STD))\n",
        "  model.apply(print_model_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6f0aQjxhHc1",
        "outputId": "91da4a0c-6805-4cdc-f8b1-8c58a3984751"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MODEL WEIGHTS SUMMARY ===\n",
            "\n",
            "Layer:  (Linear)\n",
            "  Weight shape: torch.Size([300, 500])\n",
            "  Bias shape:   torch.Size([300])\n",
            "  First 10 weight values: [0.04743964225053787, 0.015163221396505833, -0.005545163527131081, -0.02757834643125534, -0.02244647778570652, -0.03231453150510788, -0.004204367287456989, 0.01719009317457676, 0.036971889436244965, -0.018321221694350243]\n",
            "  Mean: 0.000028, Std: 0.019991\n",
            "------------------------------------------------------------\n",
            "=== END OF WEIGHTS SUMMARY ===\n",
            "\n",
            "\n",
            "=== MODEL WEIGHTS SUMMARY ===\n",
            "\n",
            "Layer:  (Linear)\n",
            "  Weight shape: torch.Size([150, 300])\n",
            "  Bias shape:   torch.Size([150])\n",
            "  First 10 weight values: [-0.011043897829949856, -0.008502555079758167, -0.01630784012377262, -0.01624302938580513, -0.014607866294682026, 0.021389169618487358, 0.020934993401169777, 0.003310202853754163, -0.016984982416033745, 0.02422950230538845]\n",
            "  Mean: 0.000008, Std: 0.020052\n",
            "------------------------------------------------------------\n",
            "=== END OF WEIGHTS SUMMARY ===\n",
            "\n",
            "\n",
            "=== MODEL WEIGHTS SUMMARY ===\n",
            "\n",
            "Layer:  (Linear)\n",
            "  Weight shape: torch.Size([50, 150])\n",
            "  Bias shape:   torch.Size([50])\n",
            "  First 10 weight values: [-0.008523104712367058, 0.022459404543042183, -0.0045799370855093, 0.004297005012631416, -0.034631405025720596, 0.01506870612502098, 0.04552911967039108, 0.032495226711034775, -0.005002748686820269, 0.005717456340789795]\n",
            "  Mean: 0.000157, Std: 0.019955\n",
            "------------------------------------------------------------\n",
            "=== END OF WEIGHTS SUMMARY ===\n",
            "\n",
            "\n",
            "=== MODEL WEIGHTS SUMMARY ===\n",
            "\n",
            "Layer: linear_layer1 (Linear)\n",
            "  Weight shape: torch.Size([300, 500])\n",
            "  Bias shape:   torch.Size([300])\n",
            "  First 10 weight values: [0.04743964225053787, 0.015163221396505833, -0.005545163527131081, -0.02757834643125534, -0.02244647778570652, -0.03231453150510788, -0.004204367287456989, 0.01719009317457676, 0.036971889436244965, -0.018321221694350243]\n",
            "  Mean: 0.000028, Std: 0.019991\n",
            "------------------------------------------------------------\n",
            "\n",
            "Layer: linear_layer2 (Linear)\n",
            "  Weight shape: torch.Size([150, 300])\n",
            "  Bias shape:   torch.Size([150])\n",
            "  First 10 weight values: [-0.011043897829949856, -0.008502555079758167, -0.01630784012377262, -0.01624302938580513, -0.014607866294682026, 0.021389169618487358, 0.020934993401169777, 0.003310202853754163, -0.016984982416033745, 0.02422950230538845]\n",
            "  Mean: 0.000008, Std: 0.020052\n",
            "------------------------------------------------------------\n",
            "\n",
            "Layer: output_layer (Linear)\n",
            "  Weight shape: torch.Size([50, 150])\n",
            "  Bias shape:   torch.Size([50])\n",
            "  First 10 weight values: [-0.008523104712367058, 0.022459404543042183, -0.0045799370855093, 0.004297005012631416, -0.034631405025720596, 0.01506870612502098, 0.04552911967039108, 0.032495226711034775, -0.005002748686820269, 0.005717456340789795]\n",
            "  Mean: 0.000157, Std: 0.019955\n",
            "------------------------------------------------------------\n",
            "=== END OF WEIGHTS SUMMARY ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccoXov9Q09ce",
        "outputId": "c5f448d1-4906-4663-947b-f3c22080190d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # need stochastic gradient descent optimizer because data will be batched\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=LEARNING_RATE,\n",
        "      betas=(MOMENTUM, 0.999),\n",
        "      eps=1e-8,\n",
        "      weight_decay=WEIGHT_DECAY  # or AdamW with 0.01\n",
        "  )\n",
        "  criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "vadE75HaiOj5"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# always split first\n",
        "train_X, train_y = train_split\n",
        "val_X, val_y = val_split\n",
        "test_X, test_y = test_split"
      ],
      "metadata": {
        "id": "CoStmbySd8wF"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_X shape {train_X.shape} train_y shape {train_y.shape}\")\n",
        "print(f\"val_X shape {val_X.shape} val_y shape {val_y.shape}\")\n",
        "print(f\"test_X shape {test_X.shape} test_y shape {test_y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljcLrsEnjFXn",
        "outputId": "220264e2-43ad-4ca4-eb8f-2203e8094747"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X shape torch.Size([69229, 500]) train_y shape torch.Size([69229])\n",
            "val_X shape torch.Size([23060, 500]) val_y shape torch.Size([23060])\n",
            "test_X shape torch.Size([23117, 500]) test_y shape torch.Size([23117])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normliaze\n",
        "train_X_log1p = log1p_transform(train_X)\n",
        "val_X_log1p = log1p_transform(val_X)\n",
        "test_X_log1p = log1p_transform(test_X)"
      ],
      "metadata": {
        "id": "ET3KXQRSf6IN"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fd0RtVuL1HM0"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = create_dataloaders(train_X_log1p, train_y, val_X_log1p, val_y, test_X_log1p, test_y, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "S8LS5wEvh7SO"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLlkb-VS1RFt",
        "outputId": "b57d3d8f-2173-4787-d2cf-10a464b9ab71"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "541"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def experiment_with_fully_connected(train_loader, val_loader, test_loader, optimizer, loss, NUM_EPOCHS):\n",
        "  history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, device=device)\n",
        "\n",
        "  plot_training_history(history)\n",
        "\n",
        "  # 8. Evaluate on test split\n",
        "  test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "  print(f\"\\nFinal TEST Loss: {test_loss:.4f} | TEST Acc: {test_acc:.4f}\")\n",
        "\n",
        "  # checking what classes are misclassified the most\n",
        "  classified_dict, misclassified_dict = count_misclassifications_per_label(model, test_loader, device)\n",
        "\n",
        "\n",
        "  # misclassified_heap = []\n",
        "  # for key, value in misclassified_dict.items():\n",
        "  #   heapq.heappush(misclassified_heap, (-value, {key:key, value:value}))\n",
        "\n",
        "  # classified_heap = []\n",
        "  # for key, value in classified_dict.items():\n",
        "  #   heapq.heappush(classified_heap, (-value, {key:key, value:value}))\n",
        "  # Sorted by count descending\n",
        "  sorted_misclassified = sorted(\n",
        "    misclassified_dict.items(), key=lambda x: x[1], reverse=True\n",
        "  )\n",
        "\n",
        "  sorted_classified = sorted(\n",
        "    classified_dict.items(), key=lambda x: x[1], reverse=True\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  config = {\n",
        "    \"is_init_weights_normal\":IS_INIT_WEIGHTS_NORMAL,\n",
        "    \"mean\":INIT_WEIGHTS_MEAN,\n",
        "    \"std\":INIT_WEIGHTS_STD,\n",
        "    \"train_ratio\": TRAIN_RATIO,\n",
        "    \"val_ratio\": VAL_RATIO,\n",
        "    \"test_ratio\": TEST_RATIO,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"weight_decay\": WEIGHT_DECAY,\n",
        "    \"momentum\":MOMENTUM,\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"optimizer\": optimizer.__class__.__name__,\n",
        "    \"loss_function\": criterion.__class__.__name__,\n",
        "    \"num_classes\":NUM_CLASSES,\n",
        "    \"model_name\": model.__class__.__name__,\n",
        "    \"device\": str(device),\n",
        "\n",
        "    \"layers\":f\"nn.Linear({500},300) -> nn.Relu -> nn.Linear(300,150)->nn.Relu -> nn.Linear(150,50)\"\n",
        "  }\n",
        "\n",
        "  test_results = {\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_acc,\n",
        "    \"num_test_samples\": int(test_X.shape[0]),\n",
        "    \"classification_count_by_label\": [\n",
        "        {\"label\": int(k), \"count\": int(v)} for k, v in sorted_classified\n",
        "    ],\n",
        "    \"misclassification_count_by_label\": [\n",
        "        {\"label\": int(k), \"count\": int(v)} for k, v in sorted_misclassified\n",
        "    ],\n",
        "\n",
        "  }\n",
        "\n",
        "  log_experiment(\n",
        "    config=config,\n",
        "    history=history,\n",
        "    test_results=test_results,\n",
        "    log_root=log_root  # folder in your drive/colab FS\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sqHcuOFnZqlB"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive_karim')\n",
        "experiment_with_fully_connected(train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,optimizer=optimizer, loss=criterion, NUM_EPOCHS=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "TjZKbXe-eDE0",
        "outputId": "8841abf6-cd53-4c90-bcd8-622014c3e0ea"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive_karim; to attempt to forcibly remount, call drive.mount(\"/content/drive_karim\", force_remount=True).\n",
            "Epoch [1/100] Train Loss: 3.9120 | Train Acc: 0.0173 | Val Loss: 3.9120 | Val Acc: 0.0192\n",
            "Epoch [2/100] Train Loss: 3.9120 | Train Acc: 0.0173 | Val Loss: 3.9120 | Val Acc: 0.0192\n",
            "Epoch [3/100] Train Loss: 3.9120 | Train Acc: 0.0173 | Val Loss: 3.9120 | Val Acc: 0.0192\n",
            "Epoch [4/100] Train Loss: 3.9120 | Train Acc: 0.0173 | Val Loss: 3.9120 | Val Acc: 0.0192\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2660065574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive_karim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexperiment_with_fully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1817647935.py\u001b[0m in \u001b[0;36mexperiment_with_fully_connected\u001b[0;34m(train_loader, val_loader, test_loader, optimizer, loss, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexperiment_with_fully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2337014090.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mval_acc\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-941348640.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# inputs: (batch, H, W) -> add channel dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#inputs = inputs.unsqueeze(1).to(device)   # (batch, 1, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBvrzEZJuUyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}